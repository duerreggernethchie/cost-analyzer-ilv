{
  
    
        "post0": {
            "title": "Lessons learned and discussion",
            "content": "Lessons learned and discussion . Zelenne Huhn . Summary . This course (ILV and LAB) was for me for sure the most difficult so far in the master and overall in my studies. It was at times very frustrating to pay all my attention and having the feeling of not understanding anything. . It was interesting for me to see how much I could learn and understand because the compromise with my team was so big that I didn’t want to disappoint them. I learned as well that I don’t need to know all details and don’t need to be an expert in all subjects since our team has different roles and everyone of us has his specialty. . It was awesome to see that my team is really willing to support each other by being patient, helping, answering questions, etc. I am really happy to have experienced this since this time I was the one needing help. . I found the quizzes a very good idea! I really liked them because it gave me the chance to refresh in a very easy and fast way the most important concepts from the course. . Thanks to this course, many concepts and words aren’t strange to me anymore and I know where to look at when I will be confronted again with this subject. . For me is clear that this course would have been way easier if I would have knowledge of programming, not just the basics. For this reason, during the summer of 2020 I will invest a lot of time learning python. . Discussion . Requirements engineering was for me the easiest part from this course. I’ve done this for the company I work for and even if we did this as good as we could, we had to do a lot of changes. A system should be adapting itself to the new environments the whole time. As well, it is very complicated to be able to see every single detail when a tool doesn’t exist yet. . Web basics was as well easy to follow. These concepts or words are to all known but if you are not confronted with them, it’s not easy to know what is meant with it. Would be very cool to know what is the difference between all front ends and platforms for the frameworks, why are there so many? Which is the best? Why? These questions would be great to get answered but maybe it’s not so important if you are not doing this as a daily basis. . Software Architecture and design are without a doubt key in every software development project. These can be so complicated that without specialists or a good project management, such a project can be a disaster. . Nethchie Dürregger . Summary: . The software development course is by far the most challenging subject for me but at the same time also the most fullfilling so far. I have always been scared about it before having close to zero knowledge of programming and development. There are lot of terms and technologies that i did not know they exist, i did not know what it is for and did not know how to apply. It’s a great realizing that there are so much more to learn everyday. Although the learning process can be very frustrating, atleast i feel very frustrated quite lot of times when i do not understand the topic or things are not working that i even find myself staring at my device for a minute or two clicking the mouse anywhere while feeling lost not only once but many times. But thanks for all the provided articles and tutorials, they really guide me through solving problems like debugging code etc. It is fullfilling for many reasons. Not only that i learn lot of things, lot of new technologies, but the thought that I together with my team are brainstorming concepts and developing an app. It’s a dream come true. Although i do not see myself into this direction just yet because i still need to learn lot of things but getting there as i am learning everyday. . I also would also like to highlight the teamwork. The course does not only teach me technical skills but also soft skill. The idea of having a team realy molded me into a great teamplayer. It is embarassing to admit that i am not good at asking for help (maybe because in the environment where i am moving, there is an unbalance give and take) Self-reliance is all i know. With all the challenges i have encountered in software development, i learned to ask for help from my team and others and started exchanging ideas on what is best for our project/portfolios. My team is great. We mostly communicate via teams and whatsapp. We work well together. Helping each other for the success of the project concept and implementation. Thank you to Lukas Huber and Michael Erharter my Professors who are always willing to extend their helping hand to the struggling but interested to learn students. . Discussion: . As almost everything is new to me, it is interesting to know and learn not only the requirements engineering, but also design and architectural part of developing a software, i have to admit that i find it very complicated to understand in the beginning but once i dig myself into it and thorough understanding it, I am now able to create different UML diagrams to show relationships of the central elements of the system, and planning of how an application could look like in a dashboard and all the needed requirements. Of course there’s a lot of rooms for improvement but getting there slowly. I like the web basics as well, playing around with HTML and as well as deploying our portfolios online. I also like continuous integration, delivery and deployment because it allows me to integrate small pieces of code at one time, failures are detected faster and can be repaired faster, and cotinuous testing my code if the outputs are correct. The app depployment in the kubernetes cloud is a challenging and interesting for me as well. I was very curious before about how could i deploy my own data science educational blogs that i wrote while learning, for my easy access when i need it. Great to know it is possible through githubs fastpages or gitlab pages. During the summer break, it is my goal to create my own app for practice purpose and be more confident with software development. . Tomasz Jendrzyca . Summary . Software development is for me the hardest and mostly misterious area from our Data Science studies (at least in the first 2 semesters), so to be honest I had some fear regarding this project. Thanks to my collegues and they support it was possible to create this project and to contribute to it to some extend. . The idea of our project is to build a Cost analyser application. I will not concentrate here on the details which can be found in the documents we created and shared for this project. In this summary I would like to describe the lessons learned and discuss our results and the outcome of the course from my point of view. . Lessons Learned and Discussion . Requirements Engineering was from my side the easiest part of the course to understand, at least theoretically. The correct and state of the art implementation seems to be already much harder. For somebody, who never wrote such requirements it is hard to decide what does it mean for instance specific enough formulation; for instance this sentence: “The system will be offering different standard layout according to the preferences of the customers.” It is specific enough? But how specific should it be? For me here the lesson learned is to think about the rules we have learned one should stick to writing the requirements. I think that only after some experience it is possible to do it well. . Writing about the architecture was for me an interesting expierience, I have had really no/little understanding about this technical issues and what one has to implement so that an architecture makes sense and functions well. So I tried to analyse what we have learned during the course ( or what I undestood from it) and I googled so that an architecture could have been created for our application and at least in my understanding could work well and help to fulfill the requriements of our project. What I could learned from it, is the fact that one has to concentrate on the requirements thinking about an architecture, besides that a lot of ideas and concepts were here new for me and so I had a possibility to learned them also in an application form. . Similar experience as described above was connected with defining and describing the data science architecture. It was a process of trying to understsand the material from the course and implement it to our application. It was again quite chellanging to think how it could work but also suprisingly quite interesting. . For me the big lesson learned is that starting doing the projects nothing seems understandable for me and after that I`m still at the beginning but at least I see that it is not everything here a rocket science as it seems before. . I think that for someone, who has no previous expierience (besides Sofware Development I and Data Engineering) in the topics of Software Development and never worked in this area, the course was quite chellanging, this was my expierience. . It was very helpfull that we have in the group collegues with much more expierience and better understanding in the area so one could ask for help and exchange the ideas. . So after the course I have the feeling to learned some important and interesting concepts. I think I can connect them and tell why they matters and build with them some theoretical solutions. For me even more chellanging is the application of this concepts (LAB). . Christian Brandl . It was very interesting to work on this project Cost Analyser App to create more requirements and generate insights on new features. Basically I have already heard everything from the lecture, but often had no time to have a deeper look. To work in a heterogeneous team is very nice, because it get input from different sites. The Lab for Software Development was very challenging, time consuming, but very interesting to work with all the topics from the ILV. Designing a software is alway a very nice work and it would be possible to even find much more topic to specify in this project and for the Cost Analyser. Due to that we worked already a lot in the Lab we decided to have a focus on the theoretical parts of designing and architecting a software. . We had a very good communication via Teams and Whats-App. We started working on Word and PPT documents, until we realized to provide the results via a GitLab. Therefore we switched to the GitLab repository https://gitlab.web.fh-kufstein.ac.at/christian.brandl/csv-cost-analyser-app-ilv/ and used also tickets to structure the goals and asign the tasks to different persons in the team. This worked out very well. . Lessons Learned and Discussion . From the theory it was very useful to get an update on design patterns with good examples. This topic is always very difficult to understand and bring it then into real world scenarios. . After the course I have got an update of important and interesting modern concepts. The project was very useful for persons new to this topic to get a fast overview of the needed tasks to specify a software development project for developers and also get an overview of the needed roles of a project team. For me it was a very good summary and reminder on modern architecture and also to use this again in my daily business. . The big lesson learned for me was to get an update on best practice software design with modern tools. Best thinks for me where Data Science Architektures and working with Pipelines. This is a very interesting topic for me and I am looking forward to use this more in practice. Also Container, Kuberenets and Cloud is something a modern software developer has to know. . Markus Auer . Summary . This course was not easy to transfer into the project despite my experiences from several programming and software engineering courses. It is different to the other courses as you can not learn the content to use it like programming in different languages. . My strenghts are lying more in the practical part. Taking over the role as a PO in the LAB was good to practice speaking and selling improvements of the progress. The downside was that I was not able to dive into the several new elements we used. . The part of the web technologies was complete new to me. Seeing what single elements are adding value to the project and how fast they can be used motivates to use them for own projects. . Lessons Learned and Discussion . We created a very long list of requirements for a tool that seems simple to describe. Such a length can produce a impression of having a complete list of requirements. Maybe a project should define a milestone where the application is ready to be released not when it is done with all its features. For sure the requirements are changing over the project progress but they can be integrated easier. This leads to a discussion of the kind of project management. . For me I can say that I now know some of the terms of the daily growing universe. In the first moment I resignated not to understand what Docker, Kubernetes and all other new thing mean in theory. Seeing how they are running makes it much more interesting to work with. .",
            "url": "https://duerreggernethchie.github.io/cost-analyzer-ilv/2020/11/07/Lessons-learned-and-discussion.html",
            "relUrl": "/2020/11/07/Lessons-learned-and-discussion.html",
            "date": " • Nov 7, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Machine Learning",
            "content": "Table of Contents . 1&nbsp;&nbsp;Business Understanding | 2&nbsp;&nbsp;Data Loading | 3&nbsp;&nbsp;Data Wrangling3.1&nbsp;&nbsp;GroupBy | 3.2&nbsp;&nbsp;Filter Condition | 3.3&nbsp;&nbsp;Unique | . | 4&nbsp;&nbsp;Data Understanding4.1&nbsp;&nbsp;Head, Describe, Info, ISNA, ANY | 4.2&nbsp;&nbsp;Pairplot (Seaborn) | 4.3&nbsp;&nbsp;Clustermap (seaborn) | 4.4&nbsp;&nbsp;Histogram of all Variables | 4.5&nbsp;&nbsp;Histogram Single Variables | 4.6&nbsp;&nbsp;RealMap Plot | 4.7&nbsp;&nbsp;Countplot (seaborn) | 4.8&nbsp;&nbsp;HeatMap (seaborn) | 4.9&nbsp;&nbsp;Boxplot (Seaborn) | 4.10&nbsp;&nbsp;Barplot (seaborn) | 4.11&nbsp;&nbsp;Scatterplot(Seaborn) | 4.12&nbsp;&nbsp;Lineplot (seaborn) | . | 5&nbsp;&nbsp;Überlegungen zu unseren Daten | 6&nbsp;&nbsp;Train Test Splitting | 7&nbsp;&nbsp;Data Preprocessing7.1&nbsp;&nbsp;Bereinigen fehlender Werte | 7.2&nbsp;&nbsp;Bereinigen nicht-nummerische Werte | 7.3&nbsp;&nbsp;Bereinigung von Skalierungen7.3.1&nbsp;&nbsp;MinMaxScaler | 7.3.2&nbsp;&nbsp;RobustScaler | 7.3.3&nbsp;&nbsp;One-Hot Encoding | . | 7.4&nbsp;&nbsp;Pipeline Verarbeitung | 7.5&nbsp;&nbsp;🔧 Input Pipeline | 7.6&nbsp;&nbsp;🔧 Output Pipeline | 7.7&nbsp;&nbsp;⚙️ Daten durch Pipeline schicken | . | 8&nbsp;&nbsp;Modeling8.1&nbsp;&nbsp;Linear Regression | 8.2&nbsp;&nbsp;Regularization Techniques8.2.1&nbsp;&nbsp;Ridge Regression | 8.2.2&nbsp;&nbsp;Lasso Regression | . | 8.3&nbsp;&nbsp;Tree-Based Models8.3.1&nbsp;&nbsp;Decision Tree | 8.3.2&nbsp;&nbsp;Random Forest | 8.3.3&nbsp;&nbsp;Boosting | 8.3.4&nbsp;&nbsp;Bagging | . | 8.4&nbsp;&nbsp;Support Vector Machine | 8.5&nbsp;&nbsp;Logistic Regression | 8.6&nbsp;&nbsp;K-Nearest Neighbor | . | 9&nbsp;&nbsp;Model Comparison | 10&nbsp;&nbsp;Evaluation | 11&nbsp;&nbsp;6️⃣ Deployment | . Business Understanding . Dieser Schritt im CRISP-DM Zyklus läuft in der Regel ohne Technische Unterstützung ab. Hier geht es vorrangig darum, die Domäne in der wir uns bewegen zu verstehen. Das klingt einfach, ist es aber in der Regel nicht. Beim Kick-Off eines ML Projekts klingt vieles noch wenig vertraut und muss erst hinterfragt werden. Hier ein paar Fragen... . Was macht das Unternehmen; womit verdient es sein Geld? | Was wird mit dem aktuellen ML Projekt (wirklich) bezweckt? | Wer unterstützt das Projekt (nicht)? | Woher kommen die Daten, mit denen wir arbeiten sollen? | Wer gewährt uns Zugriff zu welchen Daten? | Warum sehen die Daten so aus, wie sie eben aussehen und nicht anders? | Welche Features können wir nutzen versus Welche Features würden wir benötigen? | Welche Möglichkeiten haben wir, unsere Daten zu erweitern? | . Data Loading . Natürlich beginnen wir unseren Machine Learning Prozess mit dem Laden der Daten. Hier verwenden wir pandas. Zum einen bietet Pandas eine Menge an Methoden, die uns beim Laden und Selektieren der Daten helfen; zum anderen aber bietet uns pandas auch einigen Support, bei der Vorverarbeitung der Daten. . import pandas as pd data = pd.read_csv(&quot;../data/housing.csv&quot;) bikesharing_data = pd.read_csv(&quot;../data/Bikesharing.csv&quot;, index_col=0) . data.median_income.mean() . 3.8706710029069766 . #other method bikesharing_data = bikesharing_data.fillna(bikesharing_data[&quot;windspeed&quot;].median()) . data.fillna(data.median_income.mean()) . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity . 0 -122.23 | 37.88 | 41.0 | 880.0 | 129.0 | 322.0 | 126.0 | 8.3252 | 452600.0 | NEAR BAY | . 1 -122.22 | 37.86 | 21.0 | 7099.0 | 1106.0 | 2401.0 | 1138.0 | 8.3014 | 358500.0 | NEAR BAY | . 2 -122.24 | 37.85 | 52.0 | 1467.0 | 190.0 | 496.0 | 177.0 | 7.2574 | 352100.0 | NEAR BAY | . 3 -122.25 | 37.85 | 52.0 | 1274.0 | 235.0 | 558.0 | 219.0 | 5.6431 | 341300.0 | NEAR BAY | . 4 -122.25 | 37.85 | 52.0 | 1627.0 | 280.0 | 565.0 | 259.0 | 3.8462 | 342200.0 | NEAR BAY | . 5 -122.25 | 37.85 | 52.0 | 919.0 | 213.0 | 413.0 | 193.0 | 4.0368 | 269700.0 | NEAR BAY | . 6 -122.25 | 37.84 | 52.0 | 2535.0 | 489.0 | 1094.0 | 514.0 | 3.6591 | 299200.0 | NEAR BAY | . 7 -122.25 | 37.84 | 52.0 | 3104.0 | 687.0 | 1157.0 | 647.0 | 3.1200 | 241400.0 | NEAR BAY | . 8 -122.26 | 37.84 | 42.0 | 2555.0 | 665.0 | 1206.0 | 595.0 | 2.0804 | 226700.0 | NEAR BAY | . 9 -122.25 | 37.84 | 52.0 | 3549.0 | 707.0 | 1551.0 | 714.0 | 3.6912 | 261100.0 | NEAR BAY | . 10 -122.26 | 37.85 | 52.0 | 2202.0 | 434.0 | 910.0 | 402.0 | 3.2031 | 281500.0 | NEAR BAY | . 11 -122.26 | 37.85 | 52.0 | 3503.0 | 752.0 | 1504.0 | 734.0 | 3.2705 | 241800.0 | NEAR BAY | . 12 -122.26 | 37.85 | 52.0 | 2491.0 | 474.0 | 1098.0 | 468.0 | 3.0750 | 213500.0 | NEAR BAY | . 13 -122.26 | 37.84 | 52.0 | 696.0 | 191.0 | 345.0 | 174.0 | 2.6736 | 191300.0 | NEAR BAY | . 14 -122.26 | 37.85 | 52.0 | 2643.0 | 626.0 | 1212.0 | 620.0 | 1.9167 | 159200.0 | NEAR BAY | . 15 -122.26 | 37.85 | 50.0 | 1120.0 | 283.0 | 697.0 | 264.0 | 2.1250 | 140000.0 | NEAR BAY | . 16 -122.27 | 37.85 | 52.0 | 1966.0 | 347.0 | 793.0 | 331.0 | 2.7750 | 152500.0 | NEAR BAY | . 17 -122.27 | 37.85 | 52.0 | 1228.0 | 293.0 | 648.0 | 303.0 | 2.1202 | 155500.0 | NEAR BAY | . 18 -122.26 | 37.84 | 50.0 | 2239.0 | 455.0 | 990.0 | 419.0 | 1.9911 | 158700.0 | NEAR BAY | . 19 -122.27 | 37.84 | 52.0 | 1503.0 | 298.0 | 690.0 | 275.0 | 2.6033 | 162900.0 | NEAR BAY | . 20 -122.27 | 37.85 | 40.0 | 751.0 | 184.0 | 409.0 | 166.0 | 1.3578 | 147500.0 | NEAR BAY | . 21 -122.27 | 37.85 | 42.0 | 1639.0 | 367.0 | 929.0 | 366.0 | 1.7135 | 159800.0 | NEAR BAY | . 22 -122.27 | 37.84 | 52.0 | 2436.0 | 541.0 | 1015.0 | 478.0 | 1.7250 | 113900.0 | NEAR BAY | . 23 -122.27 | 37.84 | 52.0 | 1688.0 | 337.0 | 853.0 | 325.0 | 2.1806 | 99700.0 | NEAR BAY | . 24 -122.27 | 37.84 | 52.0 | 2224.0 | 437.0 | 1006.0 | 422.0 | 2.6000 | 132600.0 | NEAR BAY | . 25 -122.28 | 37.85 | 41.0 | 535.0 | 123.0 | 317.0 | 119.0 | 2.4038 | 107500.0 | NEAR BAY | . 26 -122.28 | 37.85 | 49.0 | 1130.0 | 244.0 | 607.0 | 239.0 | 2.4597 | 93800.0 | NEAR BAY | . 27 -122.28 | 37.85 | 52.0 | 1898.0 | 421.0 | 1102.0 | 397.0 | 1.8080 | 105500.0 | NEAR BAY | . 28 -122.28 | 37.84 | 50.0 | 2082.0 | 492.0 | 1131.0 | 473.0 | 1.6424 | 108900.0 | NEAR BAY | . 29 -122.28 | 37.84 | 52.0 | 729.0 | 160.0 | 395.0 | 155.0 | 1.6875 | 132000.0 | NEAR BAY | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 20610 -121.56 | 39.10 | 28.0 | 2130.0 | 484.0 | 1195.0 | 439.0 | 1.3631 | 45500.0 | INLAND | . 20611 -121.55 | 39.10 | 27.0 | 1783.0 | 441.0 | 1163.0 | 409.0 | 1.2857 | 47000.0 | INLAND | . 20612 -121.56 | 39.08 | 26.0 | 1377.0 | 289.0 | 761.0 | 267.0 | 1.4934 | 48300.0 | INLAND | . 20613 -121.55 | 39.09 | 31.0 | 1728.0 | 365.0 | 1167.0 | 384.0 | 1.4958 | 53400.0 | INLAND | . 20614 -121.54 | 39.08 | 26.0 | 2276.0 | 460.0 | 1455.0 | 474.0 | 2.4695 | 58000.0 | INLAND | . 20615 -121.54 | 39.08 | 23.0 | 1076.0 | 216.0 | 724.0 | 197.0 | 2.3598 | 57500.0 | INLAND | . 20616 -121.53 | 39.08 | 15.0 | 1810.0 | 441.0 | 1157.0 | 375.0 | 2.0469 | 55100.0 | INLAND | . 20617 -121.53 | 39.06 | 20.0 | 561.0 | 109.0 | 308.0 | 114.0 | 3.3021 | 70800.0 | INLAND | . 20618 -121.55 | 39.06 | 25.0 | 1332.0 | 247.0 | 726.0 | 226.0 | 2.2500 | 63400.0 | INLAND | . 20619 -121.56 | 39.01 | 22.0 | 1891.0 | 340.0 | 1023.0 | 296.0 | 2.7303 | 99100.0 | INLAND | . 20620 -121.48 | 39.05 | 40.0 | 198.0 | 41.0 | 151.0 | 48.0 | 4.5625 | 100000.0 | INLAND | . 20621 -121.47 | 39.01 | 37.0 | 1244.0 | 247.0 | 484.0 | 157.0 | 2.3661 | 77500.0 | INLAND | . 20622 -121.44 | 39.00 | 20.0 | 755.0 | 147.0 | 457.0 | 157.0 | 2.4167 | 67000.0 | INLAND | . 20623 -121.37 | 39.03 | 32.0 | 1158.0 | 244.0 | 598.0 | 227.0 | 2.8235 | 65500.0 | INLAND | . 20624 -121.41 | 39.04 | 16.0 | 1698.0 | 300.0 | 731.0 | 291.0 | 3.0739 | 87200.0 | INLAND | . 20625 -121.52 | 39.12 | 37.0 | 102.0 | 17.0 | 29.0 | 14.0 | 4.1250 | 72000.0 | INLAND | . 20626 -121.43 | 39.18 | 36.0 | 1124.0 | 184.0 | 504.0 | 171.0 | 2.1667 | 93800.0 | INLAND | . 20627 -121.32 | 39.13 | 5.0 | 358.0 | 65.0 | 169.0 | 59.0 | 3.0000 | 162500.0 | INLAND | . 20628 -121.48 | 39.10 | 19.0 | 2043.0 | 421.0 | 1018.0 | 390.0 | 2.5952 | 92400.0 | INLAND | . 20629 -121.39 | 39.12 | 28.0 | 10035.0 | 1856.0 | 6912.0 | 1818.0 | 2.0943 | 108300.0 | INLAND | . 20630 -121.32 | 39.29 | 11.0 | 2640.0 | 505.0 | 1257.0 | 445.0 | 3.5673 | 112000.0 | INLAND | . 20631 -121.40 | 39.33 | 15.0 | 2655.0 | 493.0 | 1200.0 | 432.0 | 3.5179 | 107200.0 | INLAND | . 20632 -121.45 | 39.26 | 15.0 | 2319.0 | 416.0 | 1047.0 | 385.0 | 3.1250 | 115600.0 | INLAND | . 20633 -121.53 | 39.19 | 27.0 | 2080.0 | 412.0 | 1082.0 | 382.0 | 2.5495 | 98300.0 | INLAND | . 20634 -121.56 | 39.27 | 28.0 | 2332.0 | 395.0 | 1041.0 | 344.0 | 3.7125 | 116800.0 | INLAND | . 20635 -121.09 | 39.48 | 25.0 | 1665.0 | 374.0 | 845.0 | 330.0 | 1.5603 | 78100.0 | INLAND | . 20636 -121.21 | 39.49 | 18.0 | 697.0 | 150.0 | 356.0 | 114.0 | 2.5568 | 77100.0 | INLAND | . 20637 -121.22 | 39.43 | 17.0 | 2254.0 | 485.0 | 1007.0 | 433.0 | 1.7000 | 92300.0 | INLAND | . 20638 -121.32 | 39.43 | 18.0 | 1860.0 | 409.0 | 741.0 | 349.0 | 1.8672 | 84700.0 | INLAND | . 20639 -121.24 | 39.37 | 16.0 | 2785.0 | 616.0 | 1387.0 | 530.0 | 2.3886 | 89400.0 | INLAND | . 20640 rows × 10 columns . Data Wrangling . GroupBy . top_publishers = df.groupby(&#39;Publisher&#39;).size().sort_values(ascending=False).head(10).index.to_list() for i in top_publishers: print(f&#39;-) {i}&#39;) df[&#39;Publisher&#39;] = df[&#39;Publisher&#39;].where(df[&#39;Publisher&#39;].isin(top_publishers), &#39;other&#39;) df.groupby(&#39;Publisher&#39;).size().sort_values(ascending=False) . df_gr = df.groupby([&#39;Year&#39;,&#39;Publisher&#39;]).sum() df_gr = df_gr.drop([&#39;Rank&#39;], axis=1) df_gr.head() # change the shape of data frame from multiindex labels to column names test = df_gr.unstack(level=-1) test.columns = [&#39; &#39;.join(col).strip() for col in test.columns.values] test.head() # get the 30 publishers with the most rows of sales data from collections import Counter pub_count = Counter(df.Publisher) res = pub_count.most_common(30) # show as a bar plot pub = pd.DataFrame([lis[1] for lis in res], [lis[0] for lis in res], columns=[&#39;Count&#39;]) plt.bar(pub.index, pub.Count) plt.xticks(rotation=&#39;vertical&#39;) plt.title(&#39;Top 30 Publishers - Count of Data Points&#39;) plt.show() # create new table with Global_Sales, named by Publisher, as predictor and Year as Index gs = pd.DataFrame(df[df.Publisher.isin(pub.index)].groupby([&#39;Year&#39;,&#39;Publisher&#39;]).sum().Global_Sales) gs = gs.unstack(level=-1) gs.columns = gs.columns.get_level_values(1) gs.head() gs.isna().sum() gs = gs.fillna(0) # show sales data over time plt.figure(figsize=[16,10]) plt.plot(gs) plt.title(&#39;Global Sales of 30 Publishers with most data&#39;) plt.legend(gs.columns) . Filter Condition . selected = df[df[&quot;Critic_Category&quot;]&gt;=80] . Unique . len(df.Publisher.unique()) . Data Understanding . import seaborn as sns from matplotlib import pyplot as plt . Head, Describe, Info, ISNA, ANY . Hier haben wir einige der wichtigsten pandas methoden zusammengestellt, um sich einen schnellen Überblick über die Daten verschaffen zu können. Dazu gehören unter anderem... . head(n), um die ersten n Zeilen des Dataframes zu zeigen | describe(), um eine deskriptive Statistik der nummerischen Spalten zu erhalten | info() um Datentypinfos für alle Spalten zu bekommen | isna() um Datenfelder auf NaN-Werte hin zu prüfen (meistens nur in Kombination mit any() sinnvoll) | any() um Bool&#39;sche Spalten über eine Prüfung verdichten (True, wenn zumindest ein True-Wert enthalten) | . data.head(3) . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity . 0 -122.23 | 37.88 | 41.0 | 880.0 | 129.0 | 322.0 | 126.0 | 8.3252 | 452600.0 | NEAR BAY | . 1 -122.22 | 37.86 | 21.0 | 7099.0 | 1106.0 | 2401.0 | 1138.0 | 8.3014 | 358500.0 | NEAR BAY | . 2 -122.24 | 37.85 | 52.0 | 1467.0 | 190.0 | 496.0 | 177.0 | 7.2574 | 352100.0 | NEAR BAY | . data.describe() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value . count 20640.000000 | 20640.000000 | 20640.000000 | 20640.000000 | 20433.000000 | 20640.000000 | 20640.000000 | 20640.000000 | 20640.000000 | . mean -119.569704 | 35.631861 | 28.639486 | 2635.763081 | 537.870553 | 1425.476744 | 499.539680 | 3.870671 | 206855.816909 | . std 2.003532 | 2.135952 | 12.585558 | 2181.615252 | 421.385070 | 1132.462122 | 382.329753 | 1.899822 | 115395.615874 | . min -124.350000 | 32.540000 | 1.000000 | 2.000000 | 1.000000 | 3.000000 | 1.000000 | 0.499900 | 14999.000000 | . 25% -121.800000 | 33.930000 | 18.000000 | 1447.750000 | 296.000000 | 787.000000 | 280.000000 | 2.563400 | 119600.000000 | . 50% -118.490000 | 34.260000 | 29.000000 | 2127.000000 | 435.000000 | 1166.000000 | 409.000000 | 3.534800 | 179700.000000 | . 75% -118.010000 | 37.710000 | 37.000000 | 3148.000000 | 647.000000 | 1725.000000 | 605.000000 | 4.743250 | 264725.000000 | . max -114.310000 | 41.950000 | 52.000000 | 39320.000000 | 6445.000000 | 35682.000000 | 6082.000000 | 15.000100 | 500001.000000 | . df.describe().transpose() . data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 20640 entries, 0 to 20639 Data columns (total 10 columns): longitude 20640 non-null float64 latitude 20640 non-null float64 housing_median_age 20640 non-null float64 total_rooms 20640 non-null float64 total_bedrooms 20433 non-null float64 population 20640 non-null float64 households 20640 non-null float64 median_income 20640 non-null float64 median_house_value 20640 non-null float64 ocean_proximity 20640 non-null object dtypes: float64(9), object(1) memory usage: 1.6+ MB . data.isna().any() . longitude False latitude False housing_median_age False total_rooms False total_bedrooms True population False households False median_income False median_house_value False ocean_proximity False dtype: bool . data.corr() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value . longitude 1.000000 | -0.924664 | -0.108197 | 0.044568 | 0.069608 | 0.099773 | 0.055310 | -0.015176 | -0.045967 | . latitude -0.924664 | 1.000000 | 0.011173 | -0.036100 | -0.066983 | -0.108785 | -0.071035 | -0.079809 | -0.144160 | . housing_median_age -0.108197 | 0.011173 | 1.000000 | -0.361262 | -0.320451 | -0.296244 | -0.302916 | -0.119034 | 0.105623 | . total_rooms 0.044568 | -0.036100 | -0.361262 | 1.000000 | 0.930380 | 0.857126 | 0.918484 | 0.198050 | 0.134153 | . total_bedrooms 0.069608 | -0.066983 | -0.320451 | 0.930380 | 1.000000 | 0.877747 | 0.979728 | -0.007723 | 0.049686 | . population 0.099773 | -0.108785 | -0.296244 | 0.857126 | 0.877747 | 1.000000 | 0.907222 | 0.004834 | -0.024650 | . households 0.055310 | -0.071035 | -0.302916 | 0.918484 | 0.979728 | 0.907222 | 1.000000 | 0.013033 | 0.065843 | . median_income -0.015176 | -0.079809 | -0.119034 | 0.198050 | -0.007723 | 0.004834 | 0.013033 | 1.000000 | 0.688075 | . median_house_value -0.045967 | -0.144160 | 0.105623 | 0.134153 | 0.049686 | -0.024650 | 0.065843 | 0.688075 | 1.000000 | . Pairplot (Seaborn) . sns.pairplot(data.select_dtypes(include=[&quot;number&quot;]).dropna()) . &lt;seaborn.axisgrid.PairGrid at 0x1fdd3b1a470&gt; . Clustermap (seaborn) . sns.clustermap(data.select_dtypes(include=[&quot;number&quot;]).dropna().corr()) . &lt;seaborn.matrix.ClusterGrid at 0x1fdd7a13b70&gt; . Histogram of all Variables . data.hist(bins=50, figsize=(20,8)) plt.show() . Histogram Single Variables . titanic_data[&quot;age&quot;].plot.hist() . df.groupby(&#39;Developer&#39;).size().plot(kind=&#39;hist&#39;, bins=30, figsize=(10,10)) . RealMap Plot . import matplotlib.image as mpimg import matplotlib.cm as cm data_frame_plot = data[[ &#39;longitude&#39;, &#39;population&#39;, &#39;latitude&#39;, &#39;median_house_value&#39; ]] plt.figure(figsize=(10,10)) plt.scatter( x=data_frame_plot.longitude, y=data_frame_plot.latitude, s=data_frame_plot.population/109, c=data_frame_plot.median_house_value, alpha=0.4, cmap=&#39;plasma&#39;, marker=&quot;o&quot;, ) plt.imshow( # plt.imread(&#39;../img/california.png&#39;), extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.9 ) plt.tight_layout() #plt.savefig(&quot;../img/california_analysis.png&quot;) plt.show() . Countplot (seaborn) . sns.countplot(x=&quot;survived&quot;, data=titanic_data) sns.countplot(x=&quot;survived&quot;, hue=&quot;sex&quot;, data=titanic_data) sns.countplot(x=&quot;survived&quot;, hue=&quot;pclass&quot;, data=titanic_data) plt.figure(figsize=(10,5)) sns.countplot(x=&quot;Genre&quot;, data=vgsales_data) . HeatMap (seaborn) . sns.heatmap(titanic_data.isnull(), yticklabels=False ) . corr=vgsales_data.corr() corr = (corr) plt.figure(figsize=(10,10)) ax = sns.heatmap(corr, cbar = True, square = True, annot=True, fmt= &#39;.2f&#39;,annot_kws= {&#39;size&#39;: 10}, xticklabels=corr.columns.values, yticklabels=corr.columns.values) ax.set_title(&#39;Heatmap of Correlation Matrix&#39;) . corrMatrix = vgsales[numeric_features].corr() sns.heatmap(corrMatrix, annot=True) plt.show() . Boxplot (Seaborn) . sns.boxplot(x=&quot;pclass&quot;, y=&quot;age&quot;, data=titanic_data) . Barplot (seaborn) . sns.barplot(x=&quot;season&quot;, y=&quot;cnt&quot;, data=bikesharing_data) . # Platform overview of the when it comes to genre Genre = pd.crosstab(vgsales_data.Platform,vgsales_data.Genre) GenreTotal = Genre.sum(axis=1).sort_values(ascending = False) plt.figure(figsize=(10,10)) sns.barplot(x=GenreTotal.values,y=GenreTotal.index) . Scatterplot(Seaborn) . plt.scatter(x = &quot;mnth&quot;, y = &quot;cnt&quot;, data=bikesharing_data) . Lineplot (seaborn) . #Overview of the Global market sales from 1980 to 2020 plt.subplots(figsize=(14,6)) sns.lineplot(vgsales_data.index, y=&quot;NA_Sales&quot;, label=&quot;NA_Sales&quot;, color=&quot;red&quot;, data=vgsales_data) sns.lineplot(vgsales_data.index, y=&quot;EU_Sales&quot;, label=&quot;EU_Sales&quot;,color=&quot;blue&quot;, data=vgsales_data) sns.lineplot(vgsales_data.index, y=&quot;JP_Sales&quot;, label=&quot;JP_Sales&quot;,color=&quot;orange&quot;, data=vgsales_data) sns.lineplot(vgsales_data.index, y=&quot;Other_Sales&quot;, label=&quot;Other_Sales&quot;,color=&quot;green&quot;, data=vgsales_data) sns.lineplot(vgsales_data.index, y=&quot;Global_Sales&quot;,label=&quot;Global_Sales&quot;,color=&quot;violet&quot;, data=vgsales_data) plt.ylabel(&quot;Sales&quot;) plt.title(&quot;Video Games Sales&quot;) plt.show() . &#220;berlegungen zu unseren Daten . Woher kommen eigentlich unsere Daten? Das ist eine banale aber wichtige Frage. Meistens werden uns unsere Daten von DomänenexpertInnen zusammengestellt und meistens folgen deren Überlegungen beim Zusammenstellen der Daten einem von zwei einfachen Prinzipen: (a) Welche Daten habe ich, und/oder (b) Welche Daten könnte der Data Scientist brauchen. Das übergebene Datenset enthält aber nicht immer jene Daten, die auch gute ML Ergebnisse bringen. . Das sind die Features, die wir bekommen haben... . Längengrad (Ost-West-Lage) | Breitengrad (Nord-Süd-Lage) | Mittleres Alter in einem Zensus-Block | N der Räume in einem Zensus-Block | N Schlafzimmer in einem Zensus-Block | N der Bewohner im Zensus-Block | N Haushalte im Zensus-Block | Mittleres Einkommen (in 10.000 USD) | Mittlerer Hauswert (in USD) | Entfernung zum Meer | . Das sind die Features, die wir vielleicht brauchen... . Nächste große Stadt | Größe der nächsten Stadt | N der Städte im Einzugsgebiet | Entwicklung der nächsten, großen Stadt | Zimmer pro EinwohnerIn im Einzugsgebiet | Schlafzimmer pro EinwohnerIn im Einzugsgebiet | Personendichte (Population/Haushalte) | usw. | . Train Test Splitting . Dieser Schritt mag an dieser Stelle unpassend wirken, aber Nein! Wir spalten unsere Daten bewusst jetzt sofort in Trainings- und Testdaten auf. Wir tun das, weil wir alle weiteren Vorverarbeitungsschritte nur an unseren Trainings-Daten justieren wollen. Unsere Test-Daten legen wir zur Seite und greifen sie erst wieder an, wenn es um die Evaluierung unseres Modells geht. . Beim Splitting sind technisch gesehen sogar zwei Schritte erforderlich: Dabei sind zwei Schritte notwendig. Zum einen müssen wir unser Datenset in Input- und Output-Features spalten. Zum anderen sollten wir unsere Daten in ein Trainings- und ein Test-Datenset zerlegen. . from sklearn.model_selection import train_test_split . # trennen in input und output Features input_feature_names = [ &#39;longitude&#39;, &#39;latitude&#39;, &#39;housing_median_age&#39;, &#39;total_rooms&#39;, &#39;total_bedrooms&#39;, &#39;population&#39;, &#39;households&#39;, &#39;median_income&#39;, &#39;ocean_proximity&#39; ] output_feature_names = [&quot;median_house_value&quot;] # Datenset horizontal in input/output features splitten X = data[input_feature_names] y = data[output_feature_names] . # trennen in nummerische und nicht-numerische Features numeric_feature_names = list(X.select_dtypes(include=[&#39;number&#39;]).columns) nonnumeric_feature_names = list(X.select_dtypes(exclude=[&#39;number&#39;]).columns) # train/test Daten aufteilen X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=1239 ) . # Other method in selecting x and y variables X = bikesharing_data.drop(&quot;cnt&quot;, axis=1) y = bikesharing_data[&quot;cnt&quot;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) . # Other method , Defining Input and output features X = socr_data.drop([&quot;Weight(pounds)&quot;], axis=1) y = socr_data[[&quot;Weight(pounds)&quot;]] #Normalize features scaler = MinMaxScaler() X = scaler.fit_transform(X) y = scaler.fit_transform(y) #Train Test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) . Data Preprocessing . Hier müssen wir wir uns mit vielen Dingen beschäftigen, was uns in der Regel 80% unserer Arbeitszeit kostet. Die wichtigsten Schritte sind: . Bereinigen fehlender Werte | Bereinigen nicht-nummerische Werte | Bereinigung von Skalierungen | . Bereinigen fehlender Werte . Unser erster Schritt beim Bereinigen der Daten, ist das Bereinigen von fehlenden Werten. Wir prüfen dazu mit hilfe von pandas, wo solche Werte auftreten und wie zahlreich sie sind. Anschließend überlegen wir uns, was wir mit diesen Werten machen wollen. Hier sprechen wir unterhalb einige gängige Möglichkeiten an. . data.isna().any() . longitude False latitude False housing_median_age False total_rooms False total_bedrooms True population False households False median_income False median_house_value False ocean_proximity False dtype: bool . data.total_bedrooms.isna().sum() . 207 . Es scheint als gäbe es 207 fehlende Werte in der Spalte total_bedrooms unsers Datasets. Hier müssen wir Abhilfe schaffen, weil wir später, in der Analysephase, mit diesen Werten nicht weiterarbeiten können. Hier haben wir folgende Möglichkeiten: . Variante 1: Füllen mit einem bestimmten Wert (z.B. 0) | Variante 2: Füllen mit einem rechnerischen Wert (z.B. Arithmetisches Mittel [bei Ratioskalierten Merkmalen] oder Median [bei Ordinalen Merkmalen]) | Variante 3: Entfernen der betroffenen Datensätze | Variante 4: Bei Zeitreihen: Approximation des Wertes im Zeitverlauf (z.B. Lineare Aproximation über die beiden angrenzenden Datenpunkte) | . Variante 1 (füllen) und Variante 3 (entfernen) lassen sich sehr einfach mit Pandas umsetzen. Diese Varianten sind aber kritisch zu bewerten, weil sie dsa Datenbild entweder extrem verzerren oder uns wertvolle Daten kosten können. . # hier werden fehlende Werte z.B. mit 0 gefüllt new_dataframe = old_dataframe.fillna(0) # hier werden Zeilen mit fehlenden Werten gelöscht new_dataframe = old_dataframe.dropna() . Bereinigen nicht-nummerische Werte . Nicht-Numerische Spalten bereiten uns bei der weiteren Verarbeitung in der Regel Probleme. Darum überführen wir diese Spalten in ein nummerisches Format. Wir haben hier zwei unterschiedliche Optionen, die beide von sklearn unterstützt werden. . Variante 1 ist das Label-Encoding, bei dem jedem nicht-numerischen Wert ein numerischer Wert (0, 1, 2, 3 usw.) zugeordnet wird. | Variante 2 (und von uns präferiert) ist das One-Hot-Encoding, bei dem für jede Merkmalsausprägung eines kategorialen Attributs eine neue Spalte angelegt wird, die anschließend mit 0 oder 1 codiert wird - je nachdem, ob der Datensatz die Kategorie trägt, oder nicht. | . Label Encoding von nicht-nummerischen, kategorialen Attributen mit pandas ist ähnlich einfach, wie das füllen oder entfernen von leeren Werten: . numeric, categories = data.ocean_proximity.factorize() . Alternativ kann hier natürlich mit sklearn gearbeitet werden. Dazu wird z.B. der Tranformer LabelEncoder verwendet. Der Vorteil: Dieser lässt sich vorwärts und rückwärts anwenden (also zum codieren und decodieren). . from sklearn.preprocessing import LabelEncoder # neus Objekt der LabelEncoder-Klasse instanziieren label_encoder = LabelEncoder() # festlegen der Labels label_encoder.fit(data.ocean_proximity) # durchführen des Encodings encoded_data = label_encoder.tranform(data.ocean_proximity) # oder alternativ, beide Schritte auf einmal encoded_data = label_encoder.fit_tranform(data.ocean_proximity) . #Converting all variables into numeric. from sklearn import model_selection, preprocessing for c in vgsales_data.columns: if vgsales_data[c].dtype == &#39;object&#39;: lbl = preprocessing.LabelEncoder() lbl.fit(list(vgsales_data[c].values)) vgsales_data[c] = lbl.transform(list(vgsales_data[c].values)) vgsales_data.head(3) . Aber halt! Im Maschinellen Lernen kann es beim Label-Encoding zu Problemen kommen. Warum? Wenn wir nicht aufpassen, wird unser encodetes Label als Ordinales Merkmal verarbeitet. . Das bedeutet Kategorie 3 ist besser (weil höher) als Kategorie 2, ist besser (weil höher) als Kategorie 1 usw.. Das ist aber leider vollkommen falsch. Unser Merkmal ist eigentlich nominal und repräsentiert keine interne Ordnung. Das sagt uns später nur leider niemand mehr, und wenn wir nicht aufpassen, wird es von unserem ML Algorithmus vollkommen falsch aufgenommen. Die Lösung lautet hier One-Hot-Encoding. . from sklearn.preprocessing import OneHotEncoder # neus Objekt der OneHotEncoder-Klasse instanziieren one_hot_encoder = OneHotEncoder() # extrahieren der Werte des Dataframes als nd-array data_to_encode = data.ocean_proximity.values # umformen der Werte in eine Liste aus Einzellisten data_to_encode = data_to_encode.reshape(-1,1) # fitten und anwenden des encoders # alternativ könnten auch die methoden fit() und transform() # separat voneinander angewendet werden encoded_data = one_hot_encoder.fit_tranform(data_to_encode) . Bereinigung von Skalierungen . Features machen uns nicht nur dann Schwierigkeiten, wenn sie leere Werte enthalten. Auch die Wertebereiche einzelner Features können bei der Analyse Probleme bereiten. Unser Datenset enthält z.B. Features mit sehr hohen Werten und andere mit eher kleinen Werten. Features mit hohen Werten könnten bei der Modellierung einen höheren Stellenwert bekommen, als solche mit niedrigen Werten. Deshalb Skalieren wir unsere Features vor der Modellierung. . MinMaxScaler . MinMaxscaler: Arbeitet mit dem gesamten Wertebereich des Merkmals und ist deshalb anfällig für Ausreißer . $$s_i = dfrac{x_i–min(x)}{max(x)–min(x)}$$ . from sklearn.preprocessing import MinMaxScaler # neus Objekt der MinMaxScaler-Klasse instanziieren mima_scaler = MinMaxScaler() # fitten und anwenden des encoders # alternativ könnten auch die methoden fit() und transform() # separat voneinander angewendet werden mima_scaler.fit_transform(data[numeric_feature_names]) . #Other method Normalize Data X = bikesharing_data.drop(&quot;cnt&quot;, axis=1) y = bikesharing_data[[&quot;cnt&quot;]] scaler = MinMaxScaler() X = scaler.fit_transform(X) y = scaler.fit_transform(y) . RobustScaler . RobustScaler: Arbeitet mit dem Interquartilsabstand des Merkmals und ist deshalb robust gegenüber Ausreißer . $$s_i = dfrac{x_i–Q_1(x)}{Q_3(x)–Q_1(x)}$$ . from sklearn.preprocessing import RobustScaler # neus Objekt der RobustScaler-Klasse instanziieren rob_scaler = RobustScaler() # fitten und anwenden des encoders # alternativ könnten auch die methoden fit() und transform() # separat voneinander angewendet werden rob_scaler.fit_transform(data[numeric_feature_names]) . One-Hot Encoding . #Dummify categorical features seasons = pd.get_dummies(bikesharing_data[&quot;season&quot;], drop_first=True) weekdays = pd.get_dummies(bikesharing_data[&quot;weekday&quot;], drop_first=True) weathersits = pd.get_dummies(bikesharing_data[&quot;weathersit&quot;], drop_first=True) workingday = pd.get_dummies(bikesharing_data[&quot;workingday&quot;], drop_first=True) #concatenate the new variables to the original dataset bikesharing_data = pd.concat([bikesharing_data, seasons, weekdays, weathersits, workingday], axis=1) # Drop unnecessary variables bikesharing_data = bikesharing_data.drop(columns=[&quot;instant&quot;,&quot;dteday&quot;, &quot;season&quot;,&quot;yr&quot;, &quot;weekday&quot;,&quot;weathersit&quot;,&quot;workingday&quot;]) bikesharing_data.head() . Pipeline Verarbeitung . Wir werden im Lab unsere gesamte Vorverarbeitung mit Hilfe von Pipelines umsetzen. Das hat einige Vorteile: . Wir können die Schritte der Pipeline reproduzierbar auf Traings-, Test- und neue Daten anwenden. | Wir können die Schritte der Pipeline komfortabel rückwärts abwickln (z.B. um geschätzte Werte wieder zu skalieren. | Wir können das Pipelineobjekt einfach einfach handhaben und wiederverwenden. | from sklearn.impute import SimpleImputer from sklearn.preprocessing import OneHotEncoder, MinMaxScaler from sklearn.pipeline import Pipeline, FeatureUnion . Zur vollständigen Vorverarbeitung unserer Daten bauen wir zwei separate Pipelines für nummerische und nicht-numerische Daten und fügen diese am Ende zusammen. Jede Pipeline ist dabei gleich aufgebaut: . Zuerst werden die Pipeline-Schritte als Tupel definiert | . # als Class kommen Transformer oder Estimators in Frage step = (&quot;name_of_step&quot;, TransformerEncoderClass()) . Anschließend werden die Schritte in die Pipeline eingebaut | Dann kann die Pipeline gefittet werden (fit()) | Die einsatzbereite Pipeline kann dann mit transform() oder predict()angewendet werden | . # trennen in nummerische und nicht-numerische Features numeric_feature_names = list(X.select_dtypes(include=[&#39;number&#39;]).columns) nonnumeric_feature_names = list(X.select_dtypes(exclude=[&#39;number&#39;]).columns) . class FeatureSelector: &quot;&quot;&quot;This transformer lets you pick columns from a pandas dataset based on name :param features: List of feature names to select :type features: List of strings :param debug: Switch to send output to debug console :type debug: Boolean :raises: ValueError if features is not of type list &quot;&quot;&quot; def __init__(self, features=[]): if type(features) != list: raise ValueError(&quot;Input features must be of type List.&quot;) self.c = features def fit(self, X, y=None): &quot;&quot;&quot;This method passes-on the object as no fitting is required :param X: Input matrix :type X: Numpy matrix :param y: Output vector :type y: Numpy array :returns: self &quot;&quot;&quot; return self def transform(self, X): &quot;&quot;&quot;This method transforms the input data by selecting the features :param X: Input matrix :type X: Numpy matrix :param y: Output vector :type y: Numpy array :returns: Selected colums as Numpy matrix &quot;&quot;&quot; return X[self.c] . &#128295; Input Pipeline . X_pipeline = Pipeline([ (&quot;union&quot;, FeatureUnion([ (&quot;numeric&quot;, Pipeline([ (&quot;select_numeric_features&quot;, FeatureSelector(features=numeric_feature_names)), (&quot;replacing_missing_values&quot;, SimpleImputer(strategy=&quot;mean&quot;)), (&quot;scale_values&quot;, MinMaxScaler()) ])), (&quot;non-numeric&quot;, Pipeline([ (&quot;select_non-numeric_features&quot;, FeatureSelector(features=nonnumeric_feature_names)), (&quot;replacing_missing_values&quot;, SimpleImputer(strategy=&quot;constant&quot;, fill_value=&quot;missing&quot;)), (&quot;encode_values&quot;, OneHotEncoder()) ])) ])) ]) . &#128295; Output Pipeline . #Kohlegger method y_pipeline = Pipeline([ (&quot;scale&quot;, MinMaxScaler()) ]) #other method imputing median if target variable have missing values y_pipeline = Pipeline(steps=[ (&quot;imputer&quot;, SimpleImputer(strategy=&quot;median&quot;)), (&quot;scaler&quot;, MinMaxScaler()), ]) . &#9881;&#65039; Daten durch Pipeline schicken . X_pipeline.fit(X_train) X_train_processed = X_pipeline.transform(X_train) X_test_processed = X_pipeline.transform(X_test) . #KO method y_pipeline.fit(y_train) y_train_processed = y_pipeline.transform(y_train) y_test_processed = y_pipeline.transform(y_test) # Other methodS, reshaping variables when ko&#39;s method has error y_train_processed = y_pipeline.fit_transform(y_train.values.reshape(-1, 1)) y_test_processed = y_pipeline.fit_transform(y_test.values.reshape(-1, 1)) . Modeling . Nun, da wir unsere Daten vorbereitet haben, können wir uns der eigentlichen Analyse widmen. Wie ihr wahrscheinlich schon erkannt habt, handelt es sich beim vorliegenden Datenset um ein Datenset für supervised Learning. Darum haben wir bereits bei der Vorbereitung unsere Daten in Input- und Output-Features aufgespalten. . Linear Regression . from sklearn.linear_model import LinearRegression lr_model = LinearRegression() lr_model.fit(X_train_processed, y_train_processed) lr_model.score(X_test_processed, y_test_processed) . 0.63967411557843 . # other method Linear Modelling lr_model = LinearRegression() lr_model.fit(X_train, y_train) lr_predict = lr_model.predict(X_test) #Model Evaluation r2 = r2_score(y_test, lr_predict) mse = mean_squared_error(y_test, lr_predict) print(r2) print(mse) . #Import Linear Regression model from scikit-learn. from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score, mean_squared_error #Fit the model linreg = LinearRegression(normalize=True) linreg.fit(X_train,y_train) y_pred = linreg.predict(X_test) coef = linreg.coef_ # model metrics print(&#39;R2: &#39;, r2_score(y_test, y_pred)) print(&#39;MSE: &#39;, mean_squared_error(y_test, y_pred)) def plt_reg(y_test, y_pred, coef, modelname = &#39;Regression&#39;): plt.figure(figsize=[16, 5]) plt.suptitle(&#39;Global Sales of Namco Bandai Games with {}&#39;.format(modelname), fontsize=14) plt.subplot(121) plt.scatter(y_test.index, y_test) plt.scatter(y_test.index, y_pred) plt.title(&#39;Real vs Predicted&#39;) plt.legend([&#39;real&#39;, &#39;pred&#39;]) plt.subplot(122) plt.bar(X_train.columns, coef) plt.xticks(rotation=&#39;vertical&#39;) plt.title(&#39;Coefficients&#39;) plt.show() plt_reg(y_test, y_pred, coef, &#39;Linear Regression&#39;) . Regularization Techniques . Ridge Regression . from sklearn.linear_model import Ridge rr_model = Ridge() rr_model.fit(X_train_processed, y_train_processed) rr_model.score(X_test_processed, y_test_processed) . 0.6353167699736991 . #Fit the Ridge model ridgereg = Ridge(alpha=best_alpha,normalize=True) ridgereg.fit(X_train,y_train) y_pred = ridgereg.predict(X_test) coef = ridgereg.coef_ # model metrics print(&#39;R2: &#39;, r2_score(y_test, y_pred)) print(&#39;MSE: &#39;, mean_squared_error(y_test, y_pred)) plt_reg(y_test, y_pred, coef, &#39;Ridge Regression&#39;) . Lasso Regression . # adapted from &lt;Kornel Kielczewski -- &lt;kornel.k@plusnet.pl&gt; from sklearn.linear_model import Lasso clf = Lasso(normalize=True, max_iter=5000) coefs = [] errors = [] r_squ = [] alphas = np.logspace(-3, 1, 100) # Train the model with different regularisation strengths for a in alphas: clf.set_params(alpha=a) clf.fit(X_train, y_train) coefs.append(clf.coef_) errors.append(mean_squared_error(y_test, clf.predict(X_test))) r_squ.append(r2_score(y_test, clf.predict(X_test))) # Display results (first graph) plt.figure(figsize=(20, 6)) plt.subplot(121) ax = plt.gca() ax.plot(alphas, coefs) ax.set_xscale(&#39;log&#39;) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;weights&#39;) plt.title(&#39;Lasso coefficients as a function of the regularization&#39;) plt.axis(&#39;tight&#39;) plt.subplot(122) ax = plt.gca() ax.plot(alphas, errors) ax.plot(alphas, r_squ) ax.set_xscale(&#39;log&#39;) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;error (blue) and R2 (orange)&#39;) plt.title(&#39;Coefficient error as a function of the regularization&#39;) plt.axis(&#39;tight&#39;) plt.show() # get best alpha best_alpha = alphas[errors.index(min(errors))] print(&#39;Best Alpha: &#39;, best_alpha) # the second graph lassoreg = Lasso(alpha=best_alpha,normalize=True) lassoreg.fit(X_train,y_train) y_pred = lassoreg.predict(X_test) coef = lassoreg.coef_ # model metrics print(&#39;R2: &#39;, r2_score(y_test, y_pred)) print(&#39;MSE: &#39;, mean_squared_error(y_test, y_pred)) plt_reg(y_test, y_pred, coef, &#39;Lasso Regression&#39;) . Tree-Based Models . Decision Tree . from sklearn.tree import DecisionTreeRegressor dt_model = DecisionTreeRegressor() dt_model.fit(X_train_processed, y_train_processed) dt_model.score(X_test_processed, y_test_processed) . 0.6233226983860033 . # Other method Decision Tree dec_tree = tree.DecisionTreeRegressor() dec_tree.fit(X_train, y_train) dec_tree_predict = dec_tree.predict(X_test) #Model Evaluation r2 = r2_score( dec_tree_predict, y_test) mse = mean_squared_error(y_test, dec_tree_predict) print(r2) print(mse) . #Decision Tree Regression dec_tree = DecisionTreeRegressor(criterion=&#39;mse&#39;, max_depth=3) dec_tree.fit(X_train_p, y_train_p) dec_tree_predict = dec_tree.predict(X_test_p) #Model evaluation tree_r2 = r2_score( y_test_p, dec_tree_predict) tree_mse = mean_squared_error(y_test_p, dec_tree_predict) accuracy = round (100 - np.mean(tree_mse),2) print(&quot;accuracy: &quot;, accuracy) print(&quot;tree_r2: &quot;, tree_r2) print(&quot;tree_mse: &quot;, tree_mse) #Feature of Importance Decision tree importance = dec_tree.feature_importances_ featureList = [] for feature in X_train: featureList.append(feature) # summarize feature importance for i,v in enumerate(importance): print(featureList[i] + &quot;_&quot; , &quot;%0d: t %.5f&quot; % (i,v)) # plot feature importance plt.bar([x for x in range(len(importance))], importance) plt.show() . Random Forest . from sklearn.ensemble import RandomForestRegressor rf_model = DecisionTreeRegressor() rf_model.fit(X_train_processed, y_train_processed) rf_model.score(X_test_processed, y_test_processed) . 0.6351894261612951 . #RandomForestRegressor Modelling from sklearn.model_selection import GridSearchCV import numpy as np #y_train = y_train.ravel() rfr_model = RandomForestRegressor() rfr_model.fit(X_train_p, y_train_p) rfr_predict = rfr_model.predict(X_test_p) #Model Evaluationelasticnet_predict forest_r2 = r2_score(y_test_p, rfr_predict ) forest_mse = mean_squared_error(y_test_p, rfr_predict ) accuracy=round (100 - np.mean(forest_mse),2) print(&quot;accuracy: &quot;, accuracy) print(&quot;forest_r2: &quot;, forest_r2) print(&quot;forest_mse: &quot;, forest_mse) # plot fig, ax = plt.subplots(figsize=(14,6), dpi=60) ax.scatter(y_test.index, y_test, label=&quot; True value&quot;, color=&quot;red&quot;) ax.scatter(y_test.index, rfr_predict , label=&quot; Predicted value&quot; , color=&quot;blue&quot;) plt.legend(loc=&#39;upper left&#39;) ax.set_title(&#39;Random Forest r2 = 75% | mse = 3.14&#39;) plt.show() #Feature of Importance Random Forest tree importance = rfr_model.feature_importances_ featureList = [] for feature in X_train: featureList.append(feature) # summarize feature importance for i,v in enumerate(importance): print(featureList[i] + &quot;_&quot; , &quot;%0d: t %.5f&quot; % (i,v)) # plot feature importance plt.bar([x for x in range(len(importance))], importance) plt.show() . Boosting . #BoostingRegressor #y_train = y_train.values.ravel() boosting_model = GradientBoostingRegressor() boosting_model.fit(X_train_p, y_train_p) boosting_predict = boosting_model.predict(X_test_p) #Model Evaluation boosting_r2 = r2_score( y_test_p, boosting_predict) boosting_mse = mean_squared_error(y_test_p, boosting_predict) accuracy = round (100 - np.mean(boosting_mse),2) print(&quot;accuracy: &quot;, accuracy) print(&quot;boosting_r2: &quot;, boosting_r2) print(&quot;boosting_mse: &quot;, boosting_mse) #Feature of Importance Boosting tree importance = boosting_model.feature_importances_ featureList = [] for feature in X_train: featureList.append(feature) # summarize feature importance for i,v in enumerate(importance): print(featureList[i] + &quot;_&quot; , &quot;%0d: t %.5f&quot; % (i,v)) # plot feature importance plt.bar([x for x in range(len(importance))], importance) plt.show() # Plot fig, ax = plt.subplots(figsize=(14,6), dpi=50) ax.scatter(y_test.index, y_test_p, label=&quot;True value&quot;, color=&quot;red&quot;) ax.scatter(y_test.index, boosting_predict , label=&quot; Predicted value&quot; , color=&quot;blue&quot;) plt.legend(loc=&#39;upper left&#39;) ax.set_title(&#39;boosting r2 = 77% | mse = 2.96&#39;) plt.show() . Bagging . #BaggingRegressor #y_train = y_train.values.ravel() bagging_model = BaggingRegressor() bagging_model.fit(X_train_p, y_train_p) bagging_predict = bagging_model.predict(X_test_p) #Model Evaluation bagging_r2 = r2_score( y_test_p, bagging_predict) bagging_mse = mean_squared_error(y_test_p, bagging_predict) accuracy=round (100 - np.mean(forest_mse),2) print(&quot;accuracy: &quot;, accuracy) print(&quot;bagging_r2: &quot;, bagging_r2) print(&quot;bagging_mse: &quot;, bagging_mse) #Plot fig, ax = plt.subplots(figsize=(14,6), dpi=50) ax.scatter(y_test.index, y_test_p, label=&quot;True value&quot;, color=&quot;red&quot;) ax.scatter(y_test.index, bagging_predict , label=&quot; Predicted value&quot; , color=&quot;blue&quot;) plt.legend(loc=&#39;upper left&#39;) ax.set_title(&#39;Bagging r2 = 73.7| mse = 3.39&#39;) plt.show() . Support Vector Machine . SVR is use for classification or regression problems. For classification, it performs classification by finding the hyperplane that maximizes the margin between the two classes. The vectors (cases) that define the hyperplane are the support vectors. . from sklearn import svm #Support Vector Machine y_train = y_train.ravel() svm_model = svm.SVR() svm_model.fit(X_train, y_train) svm_model_pred = svm_model.predict(X_test) #Model Evaluation r2 = r2_score(y_test, svm_model_pred) mse = mean_squared_error(y_test, svm_model_pred) print(r2) print(mse) . #Other method svm_regressor = SVR(kernel=&#39;poly&#39;, degree=3, C=10) svm_regressor.fit(train_X, train_y) svm_regressor.score(test_X, test_y) mse = mean_squared_error(test_y.ravel(), svm_regressor.predict(test_X)) processed_mse = transformer_y.inverse_transform([[mse]]) rmse = processed_mse**.5 rmse[0][0] . Logistic Regression . #Logistic Model from sklearn.linear_model import LogisticRegression logmodel = LogisticRegression() logmodel.fit(X_train_processed, y_train) logmodel_predict = logmodel.predict(X_test_processed) #Model Evaluation from sklearn.metrics import accuracy_score accuracy = accuracy_score(y_test, logmodel_predict) #Overall Classification Report logreg_report = classification_report(y_test,logmodel_predict) print(logreg_report) . K-Nearest Neighbor . #KNeighborsRegressor Model from sklearn.neighbors import KNeighborsRegressor # can also be KNeighborsClassificator knn_model = KNeighborsRegressor() knn_model.fit(X_train_processed, y_train) knn_predict = knn_model.predict(X_test_processed) #Model Evaluation r2 = r2_score(y_test, knn_predict) mse = mean_squared_error(y_test, knn_predict) print(r2) print(mse) . Model Comparison . Evaluation . Nachdem wir mehrere . from bokeh.io import show, output_notebook from bokeh.plotting import figure from bokeh.transform import linear_cmap from bokeh.palettes import Spectral6 output_notebook() def plot_predictions_bokeh(model, X_test_processed, y_test, y_pipeline): prediction = model.predict(X_test_processed) prediction_rev = y_pipeline.inverse_transform(prediction.reshape(-1,1)) red_color = &#39;#d5042a&#39; orange_color = &#39;#ED7D31&#39; blue_color = &#39;#43bed8&#39; lightgreen_color = &#39;#98c235&#39; darkgreen_color = &#39;#0b8f6a&#39; darkblue_color = &#39;#0062A7&#39; lightblue_color = &#39;#4DBED3&#39; r_min = y_test.min()[0] r_max = y_test.max()[0] plot = figure( title=&quot;Prediction accuracy&quot;, x_axis_label=&quot;actual&quot;, y_axis_label=&quot;prdiction&quot;, x_range=[r_min, r_max], y_range=[r_min, r_max] ) plot.circle( y=prediction_rev.ravel(), x=y_test.values.ravel(), alpha=0.2, color=lightgreen_color ) plot.line( x=[r_min,r_max], y=[r_min,r_max], color=red_color ) show(plot) . Loading BokehJS ... from sklearn.metrics import mean_squared_error def plot_predictions_multi(models, X_test_processed, y_test, y_pipeline): &quot;&quot;&quot;Method plots prediction accuracy of passed models :param models: List of fitted models :param X_test_processed: Processed input data :param y_test: Unprocessed output data matching input data :param y_pipeline: y Pipeline for inverse transformation :type models: List of objects :type X_test_processed: Input matrix :type y_test: Output vector :type y_pipeline: Fitted pipeline object &quot;&quot;&quot; fig, ax = plt.subplots(1, len(models), figsize=(20,5)) i = 0 for model in models: prediction = model.predict(X_test_processed) prediction_rev = y_pipeline.inverse_transform( prediction.reshape(-1,1) ) me = round( (mean_squared_error( y_test, prediction_rev ))**0.5, 2) ax[i].scatter( x=y_test, y=prediction_rev, color=&quot;k&quot;, alpha=0.3, label=f&quot;prediction with me={me}&quot; ) ax[i].plot( [ y_test.min()[0], y_test.max()[0] ], [ y_test.min()[0], y_test.max()[0] ], color=&quot;red&quot;, ls=&quot;-&quot;, lw=4 ) ax[i].set_xlabel(&quot;acutal data&quot;) ax[i].set_ylabel(&quot;predicted data&quot;) ax[i].legend(loc=0) i += 1 . plot_predictions_bokeh( model=rf_model, X_test_processed=X_test_processed, y_test=y_test, y_pipeline=y_pipeline ) . plot_predictions_multi( models=[rf_model, lr_model, dt_model], X_test_processed=X_test_processed, y_test=y_test, y_pipeline=y_pipeline ) . 6&#65039;&#8419; Deployment . Deployment is the method by which you integrate a machine learning model into an existing production environment in order to start using it to make practical business decisions based on data. It is one of the last stages in the machine learning life cycle and can be one of the most cumbersome. Often, an organization’s IT systems are incompatible with traditional model-building languages, forcing data scientists and programmers to spend valuable time and brainpower rewriting them. . Hier haben wir euch einige Optionen aufgelistet, wie ein trainiertes ML Modell eingesetzt werden kann: . REST API: Wer selbst einen Web-Server betreibt, kann sein/ihr MLM z.B. über eine REST Schnittstelle zugänglich machen. In diesem Fall wird das Modell am Server z.B. via Python + Flask gehostet, empfängt Eingaben in Form von HTTP Requests und macht seine Ergebnisse z.B. als JSON Response zugänglich. | . Cloud Service: Wer selbst keinen Web-Server betreibt oder mehr Power hinter sein/ihr MLM bekommen möchte (Stichwort Skalierung) kann mit seinem/ihrem Modell natürlich auch auf eine Cloud-Plattform (z.B. Amazon Web Servcies, Google Cloud, Microsoft Azure, usw.) ausweichen. Alle gängigen Plattformen bieten hier geeignete Lösungen an. Manche Produkte unterstützen sogar schon in den frühen Phasen des ML Prozess und bieten vollständige Pipelines an (z.B. Microsoft in seinem Azure Machine Learning Studio). | . On-Premise Deployment: Nicht immer müssen MLM online deployt werden. Oft genügt es, ein Modell auf einem einzelnen Endgerät verfügbar zu machen. | . Noch ein Hinweis:Ein MLM ist eine mathematische Struktur - im Fall eines Regressionsmodells etwa $y = beta_0 + beta_1x_1+ beta_2x_2+...+ beta_nx_n$. Daher ist es im Grunde egal, in welcher Sprache die Daten für das Training vorbereitet und das eigentliche Training durchgeführt wurde. Das fertige Modell lässt sich praktisch in jede Sprache (Java, JavaScript, php, C, C# usw.) auslagern; sieht man einmal vom Auslagerungsaufwand ab. Online-Learning stellt hier vielleicht als einzige ML-Strategie einen Sonderfall dar. . Wir stellen uns hier vor, dass wir unser MLM über einen Web-Server zur Verfügung stellen. Hier hilft uns z.B. die Python bibliothek flask. Da wir unser MLM hier in einem Jupyter Notebook entwickelt haben, müssen wir im ersten Schritt unser Modell serialisieren. Dazu verwenden wir dill.dump(). . from dill import dump output_file = &quot;mlm_dumps/lr_model.pk&quot; # open the dedicated output file in write-binary mode with open(output_file, &quot;wb&quot;) as dump_file: dump(lr_model, dump_file) . Zusätzlich zum MLM müssen wir aber auch unsere gesamte Vorverarbeitungspipeline serialisieren - wir müssen schließlich alle neu herein kommenden Daten ebenso vorverarbeiten. Auch hier verwenden wir dill.dump(). . output_file = &quot;mlm_dumps/X_pipeline.pk&quot; # open the dedicated output file in write-binary mode with open(output_file, &quot;wb&quot;) as dump_file: dump(X_pipeline, dump_file) . output_file = &quot;mlm_dumps/y_pipeline.pk&quot; # open the dedicated output file in write-binary mode with open(output_file, &quot;wb&quot;) as dump_file: dump(y_pipeline, dump_file) . Anschließend können wir ein Skript aufsetzen, das einen flask-Web-Server erstellt und unser MLM integriert. Dieses Skript lassen wir allerdings bewusst nicht in Jupyter laufen, sondern verwenden dafür ein reguläres py-File. Sobald der Flask-Server läuft, ist das Input-Formular unter http://localhost:666/predict/form erreichbar. . &quot;&quot;&quot;This skript deploys a simple flask Web server which serves a serialized MLM :author: Michael Kohlegger :date: &quot;&quot;&quot; import pandas as pd from flask import Flask, request, render_template, jsonify from dill import load from numpy import array input_file = &quot;lr_model.pk&quot; with open(input_file, &quot;rb&quot;) as input_file: lr_model = load(input_file) input_file = &quot;X_pipeline.pk&quot; with open(input_file, &quot;rb&quot;) as input_file: X_pipeline = load(input_file) input_file = &quot;y_pipeline.pk&quot; with open(input_file, &quot;rb&quot;) as input_file: y_pipeline = load(input_file) def create_prediction( longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, dist_lt_1h_ocean, dist_inland, dist_island, dist_near_bay, dist_near_ocean ): &quot;&quot;&quot;This method creates a prediction using a serialized machine learning model :param longitude: A measure of how far west a house is; a higher value is farther west :param latitude: A measure of how far north a house is; a higher value is farther north :param housing_median_age: Median age of a house within a block; a lower number is a newer building :param total_rooms: Total number of rooms within a block :param total_bedrooms: Total number of bedrooms within a block :param population: Total number of people residing within a block :param households: Total number of households, a group of people residing within a home unit, for a block :param median_income: Median income for households within a block of houses (measured in tens of thousands of US Dollars) :param dist_lt_1h_ocean: Distance measure; 1 if &lt; 1h to the ocean. :param dist_inland: Distance measure; 1 if inland. :param dist_island: Distance measure; 1 if island. :param dist_near_bay: Distance measure; 1 if near bay. :param dist_near_ocean: Distance measure; 1 if near ocean. :return: Prediction as Float. &quot;&quot;&quot; # Create data object data = [[ longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, dist_lt_1h_ocean, dist_inland, dist_island, dist_near_bay, dist_near_ocean ]] # Scale data object data_pre = X_pipeline.transform(data) # Generate prediction # Inverse scale prediction to original prediction = y_pipeline.inverse_transform( lr_model.predict(data_pre) )[0][0] return prediction # Define Flask web service app = Flask(__name__) # Display a short message when root is called @app.route(&#39;/&#39;) def home_call(): return &quot;This is a flask Web service.&quot; @app.route(&#39;/predict&#39;, methods=[&#39;GET&#39;]) def predict(): &quot;&quot;&quot;This method creates a prediction from a http request : &quot;&quot;&quot; # Read request argurments with default = 0 longitude = float(request.args.get(&#39;longitude&#39;, &#39;0&#39;)) latitude = float(request.args.get(&#39;latitude&#39;, &#39;0&#39;)) housing_median_age = float(request.args.get(&#39;housing_median_age&#39;, &#39;0&#39;)) total_rooms = float(request.args.get(&#39;total_rooms&#39;, &#39;0&#39;)) total_bedrooms = float(request.args.get(&#39;total_bedrooms&#39;, &#39;0&#39;)) population = float(request.args.get(&#39;population&#39;, &#39;0&#39;)) households = float(request.args.get(&#39;households&#39;, &#39;0&#39;)) median_income = float(request.args.get(&#39;median_income&#39;, &#39;0&#39;)) dist_lt_1h_ocean = float(request.args.get(&#39;dist_lt_1h_ocean&#39;, &#39;0&#39;)) dist_inland = float(request.args.get(&#39;dist_inland&#39;, &#39;0&#39;)) dist_island = float(request.args.get(&#39;dist_island&#39;, &#39;0&#39;)) dist_near_bay = float(request.args.get(&#39;dist_near_bay&#39;, &#39;0&#39;)) dist_near_ocean = float(request.args.get(&#39;dist_near_ocean&#39;, &#39;0&#39;)) # Create a new prediction for the incoming data prediction = create_prediction( longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, dist_lt_1h_ocean, dist_inland, dist_island, dist_near_bay, dist_near_ocean ) output_dictionary = { &quot;output_value&quot;: prediction, &quot;message&quot;: &quot;You can expect a median house value of &quot; + str(prediction) + &quot;.&quot;, &quot;input_values&quot;: { &quot;longitude&quot;: longitude, &quot;latitude&quot;: latitude, &quot;housing_median_age&quot;: housing_median_age, &quot;total_rooms&quot;: total_rooms, &quot;total_bedrooms&quot;: total_bedrooms, &quot;population&quot;: population, &quot;households&quot;: households, &quot;median_income&quot;: median_income, &quot;dist_lt_1h_ocean&quot;: dist_lt_1h_ocean, &quot;dist_inland&quot;: dist_inland, &quot;dist_island&quot;: dist_island, &quot;dist_near_bay&quot;: dist_near_bay, &quot;dist_near_ocean&quot;: dist_near_ocean } } return jsonify(output_dictionary) @app.route(&#39;/predict/form&#39;) def form(): return render_template(&quot;input.html&quot;, title=&quot;Median House Value&quot;) # Run app if directly called if __name__ == &quot;__main__&quot;: app.run(host=&#39;localhost&#39;, port=9999, debug=True) .",
            "url": "https://duerreggernethchie.github.io/cost-analyzer-ilv/2020/08/30/Python-for-Data-Science-Lab-Compilation.html",
            "relUrl": "/2020/08/30/Python-for-Data-Science-Lab-Compilation.html",
            "date": " • Aug 30, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Requirements engineering",
            "content": "Cost Analyser Application – Requirements Engineering . [TOC] . Business Goals of the Software . The Cost Analyser application is able to help user track his bank account’s expenditures and receivables. A user will get an overview of all his expenses over several bank accounts by uploading his CSV exports. The CSV exports are downloaded from his bank accounts. The tool is able to show a dashboard with an overview of all the clustered categories (or an individual category) where the use can have an overview of his expenses. Of course the user is also able to filter by date and categories. . Figure 1: Draft of the dashboard shows a first PoC of the dashboard. Every user could have several bank accounts. As an example, the dashboard shows an account from Hypotirol, Sparkasse and Anglo Austrian Bank. Every bank account on the application shows a summary of the expenditure, receivables and balance through a line graph (line graph shows only last 3 to 6 months). However, if the user will click either button expenditure, receivable, the user has the chance to see a daily, monthly and yearly overview. And the graph next to it shows the cluster of categories (for expenditure for example: shelter, food, clothing, USW). The very left graph shows the user’s total balance of all his bank accounts. . . Main Requirements . LOG-IN SCREEN . The user should register himself in the system registration screen before he can use it, this for security reasons. The username has to be an email address and the password should be secure enough since the data is very sensitive. Password must be at least 12 characters and must contain at least one capital leter, one number and one symbol. . | In the registration process the user must give a recovery email as well as a question with an answer in order to be able to recover username or password. Optionally the user is able to add a phone number to setup a multifactor authentication with TAN codes. . | The user will have to login every time he wants to use the system (username and password). . | If the user forgets his username and/or password, there should be a button or link for the user to recover them. In this new recovery window, the user will give either the recovery e-mail or the phone number he gave for recovering when registering. The system will send a link to the recovery email for reseting username and password. When the user click on the link in the email, he will be redirected to a window where he will be able to select a new password. The username will be already filled-in in the field username. . | After the first time of login, the system will guide the user to upload his first CSV file. Hints for exporting and uploading the data will be given in form of pop-up windows or will be opened as instructions in an extra window or with a video which will need to be started by the user. . | . LOG-IN SCREEN – Possible changes . Prepare the system to log-in with best-sign technology, that means, the access should be confirmed from another confirmed device thru fingerprint. . | The system will accept other formats of documents such as xls, xlsx and others to come. For this reason, the uploading instructions must be adapted to the new comers. . | . HOME SCREEN – Main Dashboard . Like shown in Figure 1: Draft of the dashboard, a dashboard with following charts will be shown: . Balance of all accounts the customer have. . | The user should be able to select/filter which banks he/she wants to get shown. . | Overview per bank account with all receivables, expenditures and balance from the last 3 months. This can be changed and extended till 1 year. . | There should be 3 buttons to decide what should be shown, receivables, expenditures or just the balance instead of the three together. . | For the expenditures, there should be a graph where the user can see (clustered) for which classes he/she has spent how much money. . | The user should have the option to save his preferences. For example, he/she should be able to select which banks, which information he want to become shown everytime he/she is logging-in. This should be a “standard layout”. Each layout must be saved with a name. These layouts should be just shown to the owner. . | The amount of allowed layouts should be unlimited. . | Layout preferences must be saved in order to be able to suggest the user standard layouts. . | The name of the layout should be no longer than 30 characters. This information should be shown to the user in a help information window to the field. . | The user shall be able to export the dashboards as a standardized CSV or Excel report. . | . HOME SCREEN – Possible changes . New banks logos and CSV documents will be included in the software. . | The system must be able to show more dashboards which could be added in the future. . | The classes of the expenditures will be adapting themselves as more data will be collected. . | The system will be offering different standard layout according to the preferences of the customers. . | The system will be showing as well the summarized data in form of a table which will be exportable in CSV or Excel as well. . | . SECURITY . The user should be able to be logged-in just once. The system should deny other attempts to log-in a user which is already logged-in. . | The user should get a notification per E-Mail if his/her user was used at the moment he/she was already logged-in in the system. . | After 10 minutes of inactivity, the user should be asked if he/she wants to remain online. This question should be confirmed with a button “Yes, I want to remain online”. If the question is not answered in 1 minute, the system must logout the user. . | . SECURITY – Possible changes . The system should work as well in other countries. . | The system will need to be adapted to new security rules for banking software. . | . HELP MENU . There should be a FAQ section. . | In this section, the user will be able to search for the topic of his/her interest with the use of help-words. The search must not be case sensitive. . | In this menu, there should be one or more csv model documents that can give an idea to the user what kind of csv documents the system can work with. . | . HELP MENU – Posible changes . The FAQ section will be expanded constantly according to the needs of the users. . | The system will accept other formats in the future. There must be as well model documents from these other formats. . | There will be a contact formular in this section to receive questions not found in the FAQ. . | The system will use the given text in the search field by the users and will look automatically this text in the available contents in the FAQ and will suggest these questions to the user. . | . GENERAL FUNCTIONALITIES . The user should be able to log-out with just 2 clicks, no mattering where he/she is at the moment in the system. The log-out button should be confirmed with a warning window “Are you sure you want to log-out&quot; or “Finnish session”? This needs to be confirmed or denied. This will avoid that the user will be logged-out unvoluntarily in case he clicked by mistake. . | The dashboard should be as big as the browser. That means, if the user changes the size with the help of the keys “Strg” + “+” or “-“, the dashboard shown, included all information there should adapt to this new size. . | The system should be available in English and German. . | The system shall work on all main browsers. The system will be available in the internet browsers of the devices. . | The system shall work on several devices like big/small screens and iOs and Android devices. . | The system will map all uploaded CSV files to a standard model of the system. . | The user is able to upload files easily with a button “Search file”. This will open the windows explorer and a file will be able to be selected. . | The system should recognize which browser it is being opened with, and if it is not a supported browser, it should tell the user which browsers are supported. . | . GENERAL FUNCTIONALITIES – Possible changes . The system will be adapted to new internet browsers versions. . | The system should be expandible to read more formats and not just CSV. . | The system will save the information of the opened browser which was not supported in order to adapt it to these new internet browsers. . | The system will be adapted to new operating systems if needed. . | The system will be extendible to other languages. . | . NOT FUNCTIONAL ATTRIBUTES (SYSTEM PERFORMANCE) . The system should be able to show the Home-Screen dashboard after log-in in maximum 2 seconds for the average user session. . | When the user will change the actual displayed information, this changes should be shown in within a couple of seconds. . | The response time for a filter request may have no longer duration than 2 seconds for an average user session. . | . CONNECTION ISSUES . If the server would be not reachable, the system will show the user exactly this information on the screen, that the server at the moment is not available. The text to show should suggest as well to use F5 to refresh the shown information. This way, the user can understand why he is not becoming any information shown. | . SOFTWARE ISSUES . If the software has problems with loading the csv documents because of its format, the user should be informed about it. “The CSV document can’t be read, please revise it and try again. Check as well in the FAQ section for possible solutions”. . | In case of failure by uploading, the system will suggest the user to check on the standard files accepted. Links for these files will be shown right under the failure text. . | The user shall also have the possibility of requesting a new standard file from (maybe) a new bank (which is currently not supported by the system). As well, together with the failure text, there should be a link for requesting a new standard file. The link should redirect to a list with supported banks. If the user doesn’t find the desired bank in the list, there should be the option to upload a new CSV file. . | The system will send a confirmation if the file was uploaded correctly or a failure message instead. . | . SOFTWARE ISSUES – Possible changes . The system will generate a log file when failure will be detected and this will be ready to be sent to the development team per e-mail. For that, the user needs to have the chance to accept that this information will be sent. A failure window should be opened as well in which the user can accept with an “ok”. . | The system will open a window for selecting the right columns to use. The system will try to recognize which columns are needed and will suggest these to the user. Not recognized columns will be selected by the user manually. If possible, the system will try to change the format of the columns when possible. If a data column is missing, the user will know what is the problem with his file. . | . THE DATA . The data which will be saved in our DB, must be able to be exported anonym to banks for example in order to perform machine learning analysis. | . MODELLING CENTRAL ELEMENTS THROUGH UML-DIAGRAMS . At present, here is the idea of the application design through the help of different UML diagrams. . Figure 2: Class-Diagram . . The application reads the bank account CSV file specified in the FileReader Class . | Then it loads config of bank CSV file based on the IBAN . | It maps CSV fields to general data format for later use . | Finally, it calculates balance based on expenditure/receivable transactions . | . Figure 3: Sequence-Diagram . . The user uploads the bank CSV-file . | The bank informaton gets extracted into the system . | Then it gets configuration of the bank CSV file based on the IBAN . | The bank information gets structured because every bank has different header name and it is ordered differently . | The CSV mapping contains the information which columns have to be used to fill the fields in the standardized format . | The information from the CSV gets parsed and stored into the cashbook . | . Figure 4: Activity-Diagram . . This would be the activity of the Cost-Analyser App . The user’s bank CSV export is uploaded into the system . | It loads bank configs . | If the user’s CSV-file can parse, then it parses the bank data and it displays the Cashbook. . | .",
            "url": "https://duerreggernethchie.github.io/cost-analyzer-ilv/2020/07/10/Requirements-Engineering.html",
            "relUrl": "/2020/07/10/Requirements-Engineering.html",
            "date": " • Jul 10, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Software design",
            "content": "Cost Analyser Application – Software Design . [TOC] . Development Model . We will use as development waterfall model the development cycle to improve and ensure software quality. Following image will show the phases of the model: . . 1. Planning This phase will define the main goal of the development: . Setting a goal | Planning a budget | Finding the right resources to begin (developers, materials) | Scheduling | . This phase in the cycle will require a lot of communication between the development and the stakeholder(s). . 2. Requirements /Analysis This is one of the most most important steps in the cycle. The project management team must communicate with the development team. The project management team gets the requirements from the stakeholders. State of the art is an agile development modus.. Agile methods like Design Thinking may produce a backlog of tasks to be performed. . 3. Design After the requirements are discussed, agreed upon and understood the next step is for the architects of the project. The design phase requires consistency, this is why developers use frameworks! Developers will use design patterns to solve algorithmic problems. The design phase should be documented so the developers know exactly what design patterns they want to use, and where they want to use them. . 4. Implementation This phase produces the software. Agile development is very helpful because it helps organize development goals in a timely manner using “Sprints”. The project management team or the product owner should be heavily involved with this phase to ensure all expectations are met by the development team. . 5. Testing One of the most important steps in the lifecycle of software development. This phase ensures that you know if your software is of quality to deploy. Testing will help measure a number of things: . Code quality | Unit testing (functional tests) | Integration testing | Performance testing | Security testing | . 6. Deployment We will using pipelines to automate deploy the software as soon as an issue is closed (feature deployment). This will give all of the users the “finished” product until bugs are found which leads us to our next phase. . 7. Maintenance Bugs will be the biggest issue in this phase. Users will report them and it is up to the development team to fix anything that is lowering the functionality of the software. For new features we will start again with step/phase 1. . System architecture . More information about the system you can find on the document System and software Architecture . Design Patterns . Following design patterns will be used in our software: . Command Pattern . Command is a behavioral design pattern that turns a request into a stand-alone object that contains all information about the request. This transformation lets you parameterize methods with different requests, delay or queue a request’s execution, and support undoable operations. . Transactional behavior: Database engine or csv upload process may keep a list of operations that have been or will be performed. If one of them fails, all others can be reversed or discarded (usually called rollback). For example, if two database tables that refer to each other must be updated, and the second update fails, the transaction can be rolled back, so that the first table does not now contain an invalid reference. . GUI buttons and menu items: In the ability to perform the desired command, an “Action” may have an associated icon, keyboard shortcut, tooltip text, and so on. A toolbar button or menu item component may be completely initialized using only the Action object. . Bridge Pattern . An abstraction and its implementation should be defined and extended independently from each other. | A compile-time binding between an abstraction and its implementation should be avoided so that an implementation can be selected at run-time. | . For defining the mapping of our bank accounts it is necessary to have an “Abstraction” of a standard bank account to its “Implementors” like Hypo or Volksbank . Decorator Pattern . The decorator pattern can be used to extend (decorate) the functionality of a certain object statically, or in some cases at run-time, independently of other instances of the same class, provided some groundwork is done at design time. This is achieved by designing a new Decorator class that wraps the original class. . In the Cost Analyser Application we will use the Decorator Pattern also in the implementation of bank accounts and for logging within our functions. . API . An API is the base of all the apps that deal with data or enable communication between two products or services. We will empower our apps and UIs to other mobile application or platform to share its data with our or their apps/platforms. It will ease the user experience without involving the developers. . In the future, we want to provide our service also to other tools to integrate it. . Main features of our API: . Search by criteria: The API will let the users search data based on different criteria, like a date or shop name. | Paging: Many times, it happens that we do not want to see the complete data changed, but just a glimpse of it. In such a scenario, the API should be capable of determining how much data to display in one go and at what frequency. It should also inform the user about the no. of pages of data remaining. | Sorting: To ensure that the user receives all the pages of data one-by-one, the API should empower the users to sort data as per the time of modification or some other condition. | JSON Support/ REST: The REST APIs are stateless, light-weighted and let you retry the upload mobile app process if it fails. Besides, JSON’s syntax resembles that of most of the programming languages, which make it easy for a mobile app developer to parse it into any other language. | Authorization via OAuth: It is again necessary that your API authorizes via OAuth since it is faster than other methods – you just need to click on a button and it’s done. | Use Throttling: App Throttling is needed for redirecting overflow of traffic, backup APIs and safeguarding it from DoS (Denial of Service) attacks. | . HTTP Errors . Our API must be able to handle all kind of requests and return errors in as descriptive and standard form. The following are the default HTTP error codes we will use: . 200 - Generic everything is OK. | 201 - Created something OK. | 202 - Accepted but is being processed a sync (power off, provisioning, etc.). | 400 - Bad Request (invalid syntax). | 401 - Unauthorized (no current user and there should be). | 403 - The current user is forbidden from accessing this data. | 404 - That URL is not a valid route, or the item resource does not exist. | 410 – Data is deleted or doesn’t exist. | 405 - Method Not Allowed (calling post method when only get is allowed, etc.). | 500 – API internal error. | 503 - System is in maintenance mode. | . API Endpoints: . Method Description Usage . GET | Used to retrieve a representation of a resource. | GET stored data by date, shop | . POST | Used to create new new resources and sub-resources | POST new CSV file to upload a new bank export | . PUT | Used to update existing resources | PUT to update an already uploaded bank export | . PATCH | Used to update existing resources |   | . DELETE | Used to delete existing resources | Delete one or more entries | . Testing . Characteristics of a Banking Application . Before we begin testing, it’s important to note the standard features expected of any banking/finance application. A standard banking application should meet all these characteristics as mentioned below. . Support thousands of concurrent user sessions | Integration with numerous bank accounts | Process fast and secure | Include massive storage system. | To troubleshoot customer issues, it should have high auditing/logging capability | Support users on multiple platforms (Mac, Linux, Unix, Windows, mobile devices) | Support users from multiple locations | Support multi-lingual users | Support multiple service sectors (Loans, Retail banking etc.) | Foolproof disaster management mechanism | . Test Phases . Requirement Review: Quality analysts, PO, and development leads are involved in this task. The requirements document is reviewed and cross-checked to ensure that it does not affect the workflow . Database Testing: Important part of our finance application testing. This testing is done to ensure data integrity, data loading, data migration, stored procedures, and functions validation, rules testing, etc. . Integration Testing: Test that all components that are developed are integrated and validated . Functional Testing: The usual software testing activities like Test Cases. A Test Case will review and test an execution and is done during this phase . Security Testing: It ensures that the software does not have any security flaws. During test preparation, QA team needs to include both negative as well as positive test scenarios so as to break into the system and report it before any unauthorized individual access it. For Security Testing, automation tools are used while for Manual Testing tools like Proxy Sniffer, Paros proxy, HTTP watch, etc. are used . Usability Testing: It ensures that different people will be able to use the system as normal user and don’t affect the other users performance or stability. . User Acceptance Testing: It is the final stage of testing done by the end users to ensure the compliance of the application with the real world scenario. . Sample Test Cases . Admin Test Cases: . - Verify Admin login with valid and Invalid data - Verify admin login without data - Verify all admin home links - Verify admin change password with valid and invalid data - Verify admin change password without data - Verify admin change password with existing data - Verify admin logout . For Users: . - Verify all links - Verify customers login with valid and invalid data - Verify customers login without data - Verify csv upload with and without data - Verify csv upload with valid or invalid data . For new Users . - Create a new user with valid and invalid data - Create a new user without data - Verify cancel and reset option - Update user with valid and invalid data - Update user with existing data - Verify cancel option - Verify deletion of the user .",
            "url": "https://duerreggernethchie.github.io/cost-analyzer-ilv/2020/07/10/Software-Design.html",
            "relUrl": "/2020/07/10/Software-Design.html",
            "date": " • Jul 10, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "System and software architecture",
            "content": "Cost Analyser Application – System and Software Architecture . Develop a simple architecture model for your use case! . . Which architecture would you choose and why? . We decided us for the microservice architecture. We think, that this architecture can deliver desired value for our application. Furthermore they appear to be clear. As you can see in the figure above our architecture is quite simple. We want to offer two services to our customers with our application. First, to show the movements of their money. The second service is the possibility to evaluate his data simple and effektively (uploading services + analytical services). We think that for our goals the shown microservice architecture is suitable. . Which kind of interfaces are neccessary? Which desireable and which possible to implement? . For our project we decided us to use the REST API interface. Alternatively SOAP and WDL can be used. The usage of REST API offers the possibility of a good scalability as client and server are far apart. Another advantage is to integrate is easily into a web site without changing its infrastructure. . How can we use messaging? . Asynchrone messaging can be used at the analytical applications when a customer requests a new analysis that needs to train a more complex model. This request can be stored in a queue and implemented at a fitting time point. . Which information can be extracted out of log files? . In our use case we would like to extract timestamps of uploads, the bank names and from the user using the application. Furthermore all kinds of errors while the usage should be logged. We also need to know by loggin when we receive requests by our customers and the time when he received the answer. Good to know would be also the usage in form of numbers of logins and the duration he spends on the platform. . How could you stumble over internationalization - Not only because of customers of different countries? . We think that we are prepared well in this topic as our requirements are defined aswell well. Theoretically we can imagine following issues (In the first step our application is avaulable in the DACH area and GB + USA): . Different legal requirements | Different currencies | Different time zones | Metric systems with different formats | unknown letters (wrong character set) | .",
            "url": "https://duerreggernethchie.github.io/cost-analyzer-ilv/2020/07/09/System-and-Software-Architecture.html",
            "relUrl": "/2020/07/09/System-and-Software-Architecture.html",
            "date": " • Jul 9, 2020"
        }
        
    
  

  
  

  
  

  
  

  
  

  
  

  
  

  
      ,"page6": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://duerreggernethchie.github.io/cost-analyzer-ilv/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}